# Aula 5: Testes de Normalidade

## Objetivos da Aula

* Compreender a import√¢ncia dos testes de normalidade em an√°lises estat√≠sticas.
* Conhecer e aplicar os principais testes de normalidade: **Qui-quadrado**, **Kolmogorov-Smirnov**, **Shapiro-Wilk** e **Anderson-Darling**.
* Verificar se um conjunto de dados segue uma distribui√ß√£o normal.

---

## Teste de Normalidade

Os **testes de normalidade** s√£o ferramentas estat√≠sticas que permitem verificar se um conjunto de dados segue uma **distribui√ß√£o normal** ‚Äî tamb√©m conhecida como distribui√ß√£o gaussiana ou "curva em sino". Essa verifica√ß√£o √© essencial porque muitos testes estat√≠sticos cl√°ssicos, como o teste *t* ou a ANOVA, **pressup√µem que os dados analisados tenham distribui√ß√£o normal**. Quando essa condi√ß√£o n√£o √© atendida, os resultados desses testes podem ser inv√°lidos ou enganosos.

A distribui√ß√£o normal possui caracter√≠sticas espec√≠ficas, como:

* Simetria em torno da m√©dia.
* M√©dia, mediana e moda com o mesmo valor.
* 68% dos dados concentrados a um desvio padr√£o da m√©dia, 95% dentro de dois desvios e 99,7% dentro de tr√™s (regra emp√≠rica dos 3 sigmas).

### Quando utilizar testes de normalidade?

Voc√™ deve considerar aplicar testes de normalidade nas seguintes situa√ß√µes:

* **Antes de usar testes param√©tricos**, como o teste *t*, ANOVA, regress√£o linear, entre outros, pois esses m√©todos assumem que os dados s√£o normalmente distribu√≠dos.
* **Na an√°lise explorat√≥ria de dados**, para entender o comportamento geral do conjunto de dados.
* **Ao desenvolver modelos preditivos**, como regress√µes ou modelos de machine learning, em que a normalidade dos res√≠duos pode ser um requisito.
* **Na constru√ß√£o de intervalos de confian√ßa**, onde a suposi√ß√£o de normalidade melhora a precis√£o dos limites estimados.
* **Durante o pr√©-processamento**, para decidir entre aplicar transforma√ß√µes nos dados (ex.: log, raiz quadrada) ou usar testes n√£o-param√©tricos (que n√£o exigem normalidade).

> **Importante**: a normalidade dos dados pode ser avaliada visualmente (com histogramas, Q-Q plots, boxplots) ou estatisticamente (com testes formais).

---

## Qui-quadrado

O **teste Qui-quadrado de ader√™ncia** (ou de independ√™ncia, no caso de tabelas de conting√™ncia) verifica se h√° **associa√ß√£o significativa entre duas vari√°veis categ√≥ricas**. Ele compara as **frequ√™ncias observadas** com as **frequ√™ncias esperadas** sob a hip√≥tese de que as vari√°veis s√£o independentes.

### Caracter√≠sticas:

* **Tipo de dado**: qualitativo/categ√≥rico ou quantitativo categorizado.
* **Pr√©-requisito**: dados organizados em **tabela de conting√™ncia**.
* **Hip√≥teses**:

  * **H‚ÇÄ (nula)**: as vari√°veis s√£o independentes.
  * **H‚ÇÅ (alternativa)**: existe associa√ß√£o entre as vari√°veis.

### F√≥rmula:

$$
\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}
$$

Onde:

* \$O\_i\$ = frequ√™ncia observada em cada c√©lula
* \$E\_i\$ = frequ√™ncia esperada em cada c√©lula (se as vari√°veis fossem independentes)

---

### Elementos importantes do resultado (`scipy.stats.chi2_contingency`):

#### `chi2`

* Valor da estat√≠stica do teste.
* Quanto maior, maior a evid√™ncia contra a hip√≥tese nula.

#### `p`

* **p-valor**: probabilidade de obter um valor t√£o extremo quanto o observado, assumindo que H‚ÇÄ √© verdadeira.
* **Interpreta√ß√£o**:

  * Se `p < 0.05`: rejeitamos H‚ÇÄ ‚Üí existe associa√ß√£o significativa.
  * Se `p ‚â• 0.05`: n√£o rejeitamos H‚ÇÄ ‚Üí n√£o h√° evid√™ncia de associa√ß√£o.

#### `dof` (graus de liberdade)

* N√∫mero de valores livres para variar, dado um conjunto de restri√ß√µes.

* Calculado como:

$$
\text{dof} = (nLinhas - 1) \times (nColunas - 1)
$$

* Impacta diretamente a distribui√ß√£o de refer√™ncia usada para calcular o p-valor.

#### `expected`

* **Frequ√™ncias esperadas** em cada c√©lula da tabela, caso as vari√°veis fossem independentes.
* S√£o usadas na f√≥rmula do teste para comparar com as frequ√™ncias observadas.
* Calculadas por:

$$
E_{ij} = \frac{(\text{soma da linha } i) \times (\text{soma da coluna } j)}{\text{total geral}}
$$

---

### Exemplo Teste Qui-quadrado

<img src="img/5-qui_quadrado.png">

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import chi2_contingency

# Criando a tabela de conting√™ncia fict√≠cia
dados = pd.DataFrame({
    'R√°pido': [30, 20, 10],
    'M√©dio': [20, 25, 30],
    'Lento': [10, 15, 20]
}, index=['Manh√£', 'Tarde', 'Noite'])

# Teste Qui-quadrado
chi2, p, dof, expected = chi2_contingency(dados)

# Diferen√ßa entre observado e esperado
diff = dados - expected

# Figura com 2 subgr√°ficos lado a lado
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Gr√°fico 1: barras empilhadas
cores = ['#a6cee3', '#1f78b4', '#b2df8a']
dados.plot(kind='bar', stacked=True, color=cores, ax=axes[0], edgecolor='black')
axes[0].set_title('Distribui√ß√£o por Turno', fontsize=13)
axes[0].set_xlabel('Turno do Dia')
axes[0].set_ylabel('N¬∫ de Atendimentos')
axes[0].legend(title='Tempo')
axes[0].grid(axis='y', linestyle='--', alpha=0.7)
axes[0].tick_params(axis='x', rotation=0)

# Gr√°fico 2: mapa de calor da diferen√ßa (observado - esperado)
sns.heatmap(diff, annot=True, fmt=".1f", cmap="coolwarm", center=0, ax=axes[1], cbar_kws={'label': 'Desvio (Obs - Esp)'})
axes[1].set_title('Mapa de Calor: Desvios Observados x Esperados', fontsize=13)
axes[1].set_xlabel('Tempo de Atendimento')
axes[1].set_ylabel('Turno do Dia')

plt.tight_layout()
plt.show()


```

---

## Shapiro-Wilk

O **teste de Shapiro-Wilk** verifica se uma amostra de dados vem de uma **distribui√ß√£o normal**. √â especialmente recomendado para **amostras pequenas (n < 50)**, mas tamb√©m pode ser usado com tamanhos moderados.

### Caracter√≠sticas:

* **Tipo de dado**: quantitativo cont√≠nuo.
* **Hip√≥teses**:

  * **H‚ÇÄ**: os dados seguem uma distribui√ß√£o normal.
  * **H‚ÇÅ**: os dados **n√£o** seguem uma distribui√ß√£o normal.
* **Interpreta√ß√£o**:

  * Se o **p-valor < 0.05**, rejeitamos H‚ÇÄ ‚Üí os dados **n√£o** s√£o normalmente distribu√≠dos.
  * Se o **p-valor ‚â• 0.05**, **n√£o rejeitamos** H‚ÇÄ ‚Üí os dados **podem** ser considerados normais.

---

### Etapas do teste:

1. Gerar ou coletar dados.
2. Aplicar `shapiro` do `scipy.stats`.
3. Verificar o valor de W (estat√≠stica do teste) e o p-valor.
4. Usar histogramas e gr√°ficos de densidade para visualizar a distribui√ß√£o.

---

### Exemplo Shapiro-Wilk


<img src="img/5-shapiro_wilk.png">

```python
# Shapiro-Wilk - Teste de Normalidade
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import shapiro
import seaborn as sns

# Gerar dados simulados (exemplo: normal ou n√£o normal)
np.random.seed(0)
dados_normais = np.random.normal(loc=50, scale=5, size=30)
# dados_nao_normais = np.random.exponential(scale=5, size=30)

# Teste de Shapiro-Wilk
stat, p = shapiro(dados_normais)

# Resultado
print("Estat√≠stica W:", round(stat, 4))
print("p-valor:", round(p, 4))
if p < 0.05:
    print("Resultado: Os dados N√ÉO seguem uma distribui√ß√£o normal (rejeta H‚ÇÄ)")
else:
    print("Resultado: Os dados seguem uma distribui√ß√£o normal (n√£o rejeita H‚ÇÄ)")

# Gr√°fico
plt.figure(figsize=(10, 5))
sns.histplot(dados_normais, kde=True, bins=10, color='skyblue')
plt.title("Distribui√ß√£o dos Dados - Shapiro-Wilk")
plt.xlabel("Valor")
plt.ylabel("Frequ√™ncia")
plt.grid(True)
plt.tight_layout()
plt.show()
```

---

## Kolmogorov-Smirnov (K-S)

O **teste de Kolmogorov-Smirnov (K-S)** √© utilizado para comparar a distribui√ß√£o de uma amostra com uma distribui√ß√£o te√≥rica (como a normal). Ele avalia a **maior diferen√ßa entre as fun√ß√µes de distribui√ß√£o acumuladas** (CDFs) da amostra e da distribui√ß√£o de refer√™ncia.

---

### Caracter√≠sticas:

* **Tipo de dado**: quantitativo cont√≠nuo.
* **Hip√≥teses**:

  * **H‚ÇÄ**: os dados seguem a distribui√ß√£o te√≥rica especificada (ex: normal).
  * **H‚ÇÅ**: os dados **n√£o** seguem essa distribui√ß√£o.
* **Importante**: ao usar com a distribui√ß√£o normal, **os par√¢metros (m√©dia e desvio padr√£o) devem ser conhecidos**. Se forem estimados da pr√≥pria amostra, o teste deve ser ajustado com a **corre√ß√£o de Lilliefors** (o `kstest` padr√£o n√£o aplica essa corre√ß√£o).

---

### Etapas do teste:

1. Calcular a CDF da amostra.
2. Comparar com a CDF te√≥rica (ex: `norm.cdf()`).
3. Observar o valor de **D (estat√≠stica do teste)** e o **p-valor**.
4. Se necess√°rio, usar visualiza√ß√µes como histogramas ou curvas de densidade acumulada.

---

### Exemplo Kolmogorov-Smirnov

<img src="img/5-Kolmogorov_Smirnov.png">

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import kstest, norm

# Gerar dados simulados
np.random.seed(42)
dados = np.random.normal(loc=50, scale=10, size=100)

# Normalizar os dados (Z-score), j√° que o KS exige par√¢metros conhecidos
dados_norm = (dados - np.mean(dados)) / np.std(dados)

# Aplicar o teste K-S comparando com a distribui√ß√£o normal padr√£o
stat, p = kstest(dados_norm, 'norm')

# Resultados
print("Estat√≠stica D:", round(stat, 4))
print("p-valor:", round(p, 4))
if p < 0.05:
    print("Resultado: Os dados N√ÉO seguem a distribui√ß√£o normal (rejeta H‚ÇÄ)")
else:
    print("Resultado: Os dados seguem a distribui√ß√£o normal (n√£o rejeita H‚ÇÄ)")

# Gr√°fico: Distribui√ß√£o dos dados
plt.figure(figsize=(10, 5))
sns.histplot(dados_norm, kde=True, color='mediumseagreen', bins=20)
plt.title("Distribui√ß√£o dos Dados Normalizados - Teste K-S")
plt.xlabel("Z-score")
plt.ylabel("Frequ√™ncia")
plt.grid(True)
plt.tight_layout()
plt.show()
```

---

## Anderson-Darling

O **teste de Anderson-Darling** √© uma forma avan√ßada de teste de ader√™ncia, utilizado para verificar se os dados seguem uma **distribui√ß√£o espec√≠fica** (como normal, exponencial, etc.). Ele √© uma extens√£o do teste de Kolmogorov-Smirnov, por√©m **d√° mais peso √†s caudas da distribui√ß√£o** ‚Äî ou seja, ele √© **mais sens√≠vel a discrep√¢ncias nos valores muito altos ou muito baixos**.

---

### Caracter√≠sticas:

* **Tipo de dado**: quantitativo cont√≠nuo.
* **Vantagem**: mais sens√≠vel a desvios nas extremidades (caudas) da distribui√ß√£o.
* **Hip√≥teses**:

  * **H‚ÇÄ (nula)**: os dados seguem a distribui√ß√£o te√≥rica especificada.
  * **H‚ÇÅ (alternativa)**: os dados **n√£o** seguem essa distribui√ß√£o.
* Pode ser aplicado a v√°rias distribui√ß√µes al√©m da normal (exponencial, log√≠stica, etc.).

---

### Interpreta√ß√£o dos Resultados

O teste retorna:

* **Estat√≠stica A¬≤**: quanto maior for esse valor, **maior a evid√™ncia contra H‚ÇÄ**.
* Uma **tabela de valores cr√≠ticos** para diferentes **n√≠veis de signific√¢ncia**:

  * **15%**, **10%**, **5%**, **2.5%**, **1%**.

Voc√™ deve comparar o valor de **A¬≤** com esses valores cr√≠ticos.

* Se **A¬≤ for maior que o valor cr√≠tico correspondente**, **rejeita-se H‚ÇÄ** (os dados n√£o seguem a distribui√ß√£o te√≥rica).

---

### O que √© "n√≠vel de signific√¢ncia"?

O **n√≠vel de signific√¢ncia** indica **quanto risco estamos dispostos a aceitar** ao dizer que os dados **n√£o seguem uma determinada distribui√ß√£o** (e podemos estar errados).

| N√≠vel de Signific√¢ncia | O que significa na pr√°tica?                                  |
| ---------------------- | ------------------------------------------------------------ |
| **10% (0,10)**         | Mais tolerante. Aceita at√© 10% de chance de erro.            |
| **5% (0,05)**          | Mais comum. Equil√≠brio entre rigor e praticidade.            |
| **1% (0,01)**          | Muito rigoroso. S√≥ rejeita H‚ÇÄ com forte evid√™ncia nos dados. |

Se voc√™ definir o n√≠vel de signific√¢ncia como **5%**, est√° aceitando uma chance de 5 em 100 de **rejeitar H‚ÇÄ mesmo que ela seja verdadeira** ‚Äî ou seja, cometer um **erro tipo I**.

* Se a estat√≠stica **A¬≤** ultrapassa o valor cr√≠tico do n√≠vel escolhido, os dados **n√£o seguem a distribui√ß√£o te√≥rica** (rejeita-se H‚ÇÄ).
* Se **A¬≤ for menor ou igual**, ent√£o **n√£o h√° evid√™ncias suficientes para rejeitar H‚ÇÄ** ‚Äî os dados **podem ser considerados normais** (ou conforme a distribui√ß√£o testada).

---

### Exemplo Anderson Darling

<img src="img/5-Anderson_Darling.png">

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import anderson

# Gerar dados simulados
np.random.seed(42)
dados = np.random.normal(loc=100, scale=15, size=100)

# Teste de Anderson-Darling
resultado = anderson(dados, dist='norm')

# Mostrar resultados
print("Estat√≠stica A¬≤:", round(resultado.statistic, 4))
print("\nValores cr√≠ticos e signific√¢ncia:")
for critico, sig in zip(resultado.critical_values, resultado.significance_level):
    print(f"  {sig}% : {critico:.4f}")

# Veredito com base no n√≠vel de 5%
nivel_significancia = 5
indice = resultado.significance_level.tolist().index(nivel_significancia)
if resultado.statistic > resultado.critical_values[indice]:
    print("\nResultado: Os dados N√ÉO seguem uma distribui√ß√£o normal (rejeta H‚ÇÄ)")
else:
    print("\nResultado: Os dados seguem uma distribui√ß√£o normal (n√£o rejeita H‚ÇÄ)")

# Gr√°fico da distribui√ß√£o dos dados
plt.figure(figsize=(10, 5))
sns.histplot(dados, kde=True, color='slateblue', bins=20)
plt.title("Distribui√ß√£o dos Dados - Teste Anderson-Darling")
plt.xlabel("Valor")
plt.ylabel("Frequ√™ncia")
plt.grid(True)
plt.tight_layout()
plt.show()
```

---

## Projeto Pr√°tico

### **Verificando a Normalidade no Tempo de Atendimento**

Neste projeto, voc√™ ir√°:

1. Simular ou utilizar dados reais de tempo de atendimento de uma cl√≠nica (em minutos).
2. Aplicar **quatro testes de normalidade**:

   * **Shapiro-Wilk**
   * **Kolmogorov-Smirnov (K-S)**
   * **Anderson-Darling**
   * **Qui-quadrado**
3. Comparar e interpretar os resultados com base nos valores de **p** ou valores cr√≠ticos.


```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import chi2_contingency
import pandas as pd

# 1. Gerar dados simulados: tempo de atendimento (em minutos)
np.random.seed(42)
tempos = np.random.normal(loc=30, scale=5, size=100)

# 2. Visualiza√ß√£o da distribui√ß√£o
plt.figure(figsize=(10, 5))
sns.histplot(tempos, bins=10, kde=True, color='skyblue')
plt.title("Distribui√ß√£o dos Tempos de Atendimento (minutos)")
plt.xlabel("Tempo de Atendimento")
plt.ylabel("Frequ√™ncia")
plt.grid(True)
plt.tight_layout()
plt.show()

# 3. Testes de Normalidade

# Shapiro-Wilk
stat_shapiro, p_shapiro = stats.shapiro(tempos)

# Kolmogorov-Smirnov
tempos_padronizados = (tempos - np.mean(tempos)) / np.std(tempos)
stat_ks, p_ks = stats.kstest(tempos_padronizados, 'norm')

# Anderson-Darling
ad_result = stats.anderson(tempos)

# Chi2_contingency: transformar frequ√™ncias em uma matriz "artificial" de conting√™ncia
obs_freq, bins = np.histogram(tempos, bins=6)
exp_freq = len(tempos) * (stats.norm.cdf(bins[1:], loc=np.mean(tempos), scale=np.std(tempos)) -
                          stats.norm.cdf(bins[:-1], loc=np.mean(tempos), scale=np.std(tempos)))

# Organizando como matriz 2xN para o chi2_contingency
contingencia = np.array([obs_freq, exp_freq])
chi2_stat, p_chi2, dof, expected = chi2_contingency(contingencia)

# 4. Apresenta√ß√£o dos Resultados
print("=== Resultados dos Testes de Normalidade ===\n")
print(f"Shapiro-Wilk: estat√≠stica = {stat_shapiro:.4f} | p = {p_shapiro:.4f}")
print(f"Kolmogorov-Smirnov: estat√≠stica = {stat_ks:.4f} | p = {p_ks:.4f}")
print(f"Anderson-Darling: estat√≠stica = {ad_result.statistic:.4f}")
for i, sig in enumerate(ad_result.significance_level):
    crit = ad_result.critical_values[i]
    status = "REJEITA H‚ÇÄ" if ad_result.statistic > crit else "N√ÉO REJEITA H‚ÇÄ"
    print(f"  N√≠vel {sig:.1f}%: valor cr√≠tico = {crit:.4f} ‚Üí {status}")
print(f"Qui-quadrado (chi2_contingency): estat√≠stica = {chi2_stat:.4f} | p = {p_chi2:.4f}")
print(f"  Graus de liberdade = {dof}")
print("  Frequ√™ncias esperadas:")
print(np.round(expected, 2))

# 5. Resumo final
resumo = pd.DataFrame({
    "Teste": ["Shapiro-Wilk", "Kolmogorov-Smirnov", "Anderson-Darling", "Qui-quadrado (conting√™ncia)"],
    "Estat√≠stica": [stat_shapiro, stat_ks, ad_result.statistic, chi2_stat],
    "Valor-p": [p_shapiro, p_ks, "-", p_chi2],
    "Conclus√£o (p<0.05)": [
        "Rejeita H‚ÇÄ" if p_shapiro < 0.05 else "N√£o Rejeita H‚ÇÄ",
        "Rejeita H‚ÇÄ" if p_ks < 0.05 else "N√£o Rejeita H‚ÇÄ",
        "Ver n√≠veis cr√≠ticos",
        "Rejeita H‚ÇÄ" if p_chi2 < 0.05 else "N√£o Rejeita H‚ÇÄ"
    ]
})
print("\nResumo:")
display(resumo)

```

---

### **Interpreta√ß√£o dos Resultados**

Os quatro testes aplicados t√™m o objetivo de verificar se os dados do tempo de atendimento seguem uma distribui√ß√£o **normal** (aquela famosa curva em forma de sino).

* O valor de **p** (ou a estat√≠stica comparada a um valor cr√≠tico) indica **se h√° evid√™ncias contra a hip√≥tese de que os dados s√£o normais**.
* Se **p < 0,05**, ou a estat√≠stica ultrapassa o valor cr√≠tico, **rejeitamos a hip√≥tese de normalidade (H‚ÇÄ)**. Isso quer dizer que os dados provavelmente **n√£o** seguem uma distribui√ß√£o normal.
* Se **p ‚â• 0,05**, **n√£o rejeitamos H‚ÇÄ**, ou seja, **n√£o temos evid√™ncias suficientes para dizer que os dados n√£o s√£o normais** ‚Äî o que √© bom se queremos aplicar t√©cnicas que assumem normalidade.

---

### üìå **Conclus√£o Geral**

> Com base nos resultados dos quatro testes, **os dados simulados s√£o compat√≠veis com uma distribui√ß√£o normal**. Isso significa que podemos aplicar t√©cnicas estat√≠sticas que assumem normalidade (como ANOVA ou regress√£o linear), com maior confian√ßa.

Se quiser, posso complementar com uma vers√£o em Markdown estruturada para PDF ou apresenta√ß√£o. Deseja isso?

---

## Exerc√≠cios

1. Explique em suas palavras o que significa rejeitar a hip√≥tese de normalidade.
2. Simule um conjunto de dados com distribui√ß√£o **n√£o-normal** e aplique os testes.
3. Quais testes s√£o mais indicados para amostras pequenas e por qu√™?
4. O que pode acontecer se voc√™ aplicar testes param√©tricos em dados que n√£o s√£o normais?

## Materiais de Estudo Complementares

* Artigo: ["Normality Tests for Statistical Analysis: A Guide"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3693611/)
* Documenta√ß√£o do SciPy: [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html)
* Livro: "Estat√≠stica Aplicada e Probabilidades para Engenheiros", Montgomery & Runger ‚Äì Cap√≠tulo sobre Distribui√ß√µes
